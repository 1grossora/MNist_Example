{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Here we will walk through the MNIST example to learn a little more about tensorflow and how to use the framework \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)# This preloads the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we import tensorflow \n",
    "import tensorflow as tf\n",
    "#set up an interactive session \n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we will being with a single layer... \n",
    "# We need to build nodes for input images and output classes \n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])\n",
    "y_ = tf.placeholder(tf.float32,shape=[None,10])\n",
    "\n",
    "# In this case x and y_ are not specific values... just placeholders. \n",
    "#When we run TF they will be take an imput walue. \n",
    "x\n",
    "# Note that 784 is used becuase it will relate to a 28X28 pixel image \n",
    "# Note that 10 is used becuase it realates to the length of output number 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have to define weights and biases \n",
    "# We do this through variable in TF.\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10])) # we have 784 imput weights and 10 output classes\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# to bring these variables into the session we need to initialize \n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we will try to impemnt the model. We do this by taking input images * weights and then adding the bias. \n",
    "# W*x + b\n",
    "\n",
    "y = tf.matmul(x,W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we define the loss fucntion \n",
    "# loss function is a representation of how bas our model didi \n",
    "# We want to minimize the loss function\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits=y))\n",
    "# here we see that labels are the actual labels for the image \n",
    "# y is the prediction we get from the model \n",
    "#nn.softmax_cross_entropy_withLogits applies the softmax to the model which is unnoremalized \n",
    "#Then reduce_mean takes the average over the sums \n",
    "cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can train the model \n",
    "# Use gradient descent with step size 0.5\n",
    "train_step =tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# What is going on ? \n",
    "# compute gradients, compute parameter update steps, and apply update steps to the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now to train... .we are going to add a batch of photos run the training on them\\\n",
    "# We will replace x and y_ with the actual values from the given batch\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)# note 100 is batch size \n",
    "    train_step.run(feed_dict={x:batch[0],y_:batch[1]})# here we are feeding in a dictionary of the x and y_ from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well... how did we do? \n",
    "# we would like to figure out in which cases we predicted the correct label\n",
    "model_label_guess = tf.arg_max(y,1) # this is the label that our model thinks is most likely for each input\n",
    "label_true = tf.arg_max(y_,1) # This is simply the true label for the imput\n",
    "# we want to see when they are correct \n",
    "\n",
    "correct_prediction = tf.equal(model_label_guess,label_true)\n",
    "# This is a list of booleans\n",
    "\n",
    "# to findout which are correct [ True, False, True, True,False] This is then cast as floats [1.,0.,1.,1.,0.]\n",
    "# if we take the mean of this, we call this our accuracy.... how many it predicted correctly\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91720003"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want to evalue this accuracy on the test data\n",
    "\n",
    "accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-375edabfe7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# we will use zero ( it will be bad ) but it will also be a way to show random probilibty :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnstep_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#Reset a bunch of things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# lets look at the parameter space for changing a few variables. \n",
    "# here we will just use lists ( since we won't append too much ), then convert to np.arrays and plot some things \n",
    "\n",
    "# First assume step size ranges from [0,1]: plot as a function of number of training steps \n",
    "\n",
    "pretty = False\n",
    "step_size = 0.5 # just a guess to start , from experience \n",
    "batch_size = 100 # we have ten number slots 0-9 so 100 batch size should get some represenation of each number\n",
    "max_steps = 500 #max number of steps ( remember it takes longer to do more)\n",
    "nstep_step = 10#step_step\n",
    "\n",
    "# if you want to make pretty use these \n",
    "if pretty: \n",
    "    max_steps = 1000 #max number of steps ( remember it takes longer to do more)\n",
    "    nstep_step = 10#step_step\n",
    "    \n",
    "# to make life easy, let's keep this like a list of pairs [ [0_step,accuracy_0],[1_step,accuracy_1]... ]\n",
    "accuracy_for_various_n_steps = []\n",
    "\n",
    "\n",
    "# we will use zero ( it will be bad ) but it will also be a way to show random probilibty :) \n",
    "\n",
    "for n_steps in np.arange(0, max_steps, nstep_step):\n",
    "    #Reset a bunch of things\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    temp_y = tf.matmul(x,W)+b\n",
    "    temp_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits=temp_y))    \n",
    "    temp_train_step = tf.train.GradientDescentOptimizer(step_size).minimize(temp_cross_entropy)\n",
    "    for _ in range(int(n_steps)):\n",
    "        temp_batch = mnist.train.next_batch(Batch_size)\n",
    "        #Train it\n",
    "        temp_train_step.run(feed_dict={x:temp_batch[0],y_:temp_batch[1]})\n",
    "    #how many are correct? \n",
    "    temp_correct = tf.equal(tf.arg_max(temp_y,1),tf.arg_max(y_,1))\n",
    "    temp_accuracy = tf.reduce_mean(tf.cast(temp_correct,tf.float32))\n",
    "    temp_accuracy_eval = temp_accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "    temp_entry = [n_steps,temp_accuracy_eval]\n",
    "    accuracy_for_various_n_steps.append(temp_entry)\n",
    "\n",
    "print accuracy_for_various_n_steps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's take a look at how the accuracy stacks up. \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_avs():\n",
    "    avs = np.asarray(accuracy_for_various_n_steps)\n",
    "    plt.scatter(avs[1:,0],avs[1:,1])   \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "plot_avs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# a pandas DF woulc be very useful for the next part....but let's see what we can do.\n",
    "#Not everyone has pandas.... but more have numpy \n",
    "\n",
    "# So we say that somewhere around 500 steps is probably overkill \n",
    "# lets look at some more parts of parameter space \n",
    "\n",
    "#We would like to see the effect of varying batch size... maybe we simply dont have enough test samples? \n",
    "# Also: maybe the minimization is too fast or two slow.... Let's vary the step size (0.001,1)\n",
    "\n",
    "# this will take a little while to run... .but it makes a useful plot \n",
    "\n",
    "pretty = False\n",
    "n_training_steps =100 # RG CHANGE THIS BACK TO 500 \n",
    "max_step_size = 1. # descent step size\n",
    "step_size_step = 0.2 # various steps size\n",
    "max_batch_size = 100 # how many samples in training\n",
    "batch_size_step = 20 # various sample size for training \n",
    "\n",
    "b_size_list = [ z for z in range(0,max_batch_size,batch_size_step) ][1:]# remove 0 \n",
    "s_size_list = np.arange(0,max_step_size,step_size_step)# keep 0.... again more fun math\n",
    "print b_size_list\n",
    "print s_size_list\n",
    "acc_for_batch_step = [] # for list of list [ [bsize, s_size,acc]....] This might get a little heavy for append. we will see\n",
    "\n",
    "for b_size in b_size_list:\n",
    "    for s_size in s_size_list:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        temp_y = tf.matmul(x,W)+b\n",
    "        temp_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits=temp_y))    \n",
    "        temp_train_step = tf.train.GradientDescentOptimizer(s_size).minimize(temp_cross_entropy)\n",
    "        for _ in range(n_training_steps):\n",
    "            temp_batch = mnist.train.next_batch(b_size)\n",
    "            #Train it\n",
    "            temp_train_step.run(feed_dict={x:temp_batch[0],y_:temp_batch[1]})\n",
    "        #how many are correct? \n",
    "        temp_correct = tf.equal(tf.arg_max(temp_y,1),tf.arg_max(y_,1))\n",
    "        temp_accuracy = tf.reduce_mean(tf.cast(temp_correct,tf.float32))\n",
    "        temp_accuracy_eval = temp_accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels})\n",
    "        temp_entry = [b_size,s_size,temp_accuracy_eval]\n",
    "        acc_for_batch_step.append(temp_entry)\n",
    "\n",
    "print acc_for_batch_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make kde plot to see effect \n",
    "# make a scatter plot where weight is the accuracy \n",
    "def plot_bsa():\n",
    "    #plot\n",
    "    bsa = np.asarray(acc_for_batch_step)\n",
    "    plt.scatter(bsa[:,0],bsa[:,1],c=bsa[:,2])\n",
    "    plt.show()\n",
    "plot_bsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "c.eval()\n",
    "b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### CHANGE IT UP..... DO BETTER\n",
    "\n",
    "#first we need to make our own weight and bias variables \n",
    "#weights should have some noise to allow for symetrty breaking . \n",
    "# if you have all zeros you might get stuck with the gradiant and not go anywhere \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,0,1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0,1,shape=shape)# note we keep the bias positive to avoid dead nodes\n",
    "    return tf.Variable(initial)\n",
    "    \n",
    "\n",
    "#first lets define useful functions \n",
    "\n",
    "def con2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "def max_pool_2X2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now make the first convolution layer. \n",
    "# we will make is 5X5 , 1 channel with 32 features  \n",
    "# note 32 features is what's called a hyperparameter. It's just a number of features... we can make it anything\n",
    "\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "B_conv1 = bias_variable([32]) # note the number of bias is dependednt on number of features\n",
    "\n",
    "# now reshape the image to a 4d tensor\n",
    "\n",
    "x_image = tf.reshape(x,[-1,28,28,1]) # magic, dim, dim, color-chan\n",
    "\n",
    "#now we are ready to convolve the x_image tensor with the weight tensor and then add the bias and apply RelU\n",
    "\n",
    "h_conv1 = tf.nn.relu(con2d(x_image,W_conv1)+B_conv1)\n",
    "\n",
    "#then we max_pool\n",
    "h_pool1 = max_pool_2X2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cool... now we have our first layer... .we can go deeper \n",
    "# When we go deeper we will connect (bad choice of words) the previous layer to the next\n",
    "\n",
    "# this is layer 2\n",
    "\n",
    "#note the incoming image is still 14*14 because we pooled by 2X2 28/2 X 28/2\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64]) # we use 32 becuase each incoming field of view has 32 features from the previous layer\n",
    "B_conv2 = bias_variable([64])\n",
    "\n",
    "# now do the convolution and pooling \n",
    "h_conv2 = tf.nn.relu(con2d(h_pool1,W_conv2)+B_conv2)\n",
    "#now pool it \n",
    "h_pool2 = max_pool_2X2(h_conv2)\n",
    "#note... this will now give us a size 7X7 image 14/2 X 14/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now that we have a smaller image field... \n",
    "# add a fully connected layer: So the input is going to look like 7*7*64 since each pixel has now 64 features. \n",
    "#note, 32, and 64 are just arbitraty \n",
    "# so we do the same with this step These are hyper params that sweep \n",
    "\n",
    "W_fcl  = weight_variable([7*7*64,1024])#1024 is again arbitary \n",
    "B_fcl = bias_variable([1024])\n",
    "\n",
    "# since it's a fully connected layer we are going to take the previous layer and put it in to a large vector\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "#now do our normal minization of matracies \n",
    "h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat,W_fcl)+B_fcl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if we run this full layer we will have a few issues \n",
    "# first issue... it's large \n",
    "#second issue... we may over fit \n",
    "\n",
    "# to avoid this we use dropout: randomly drop out nodes before testing \n",
    "# we want to have these nodes droped in training and kept in testing \n",
    "#luckily TF does this for us. \n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fcl_drop =tf.nn.dropout(h_fcl,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finally we want to end witha  soft max.... again ranging from [0,9]\n",
    "\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "B_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fcl_drop,W_fc2)+B_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we have to just do the standard minimization technique \n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_conv))\n",
    "#Training step \n",
    "gradient_step_size = 1e-4\n",
    "train_step = tf.train.AdamOptimizer(gradient_step_size).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.arg_max(y_conv,1),tf.arg_max(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training accuracy 0.02\n",
      "step 100 training accuracy 0.26\n",
      "we are out\n"
     ]
    }
   ],
   "source": [
    "# now actually run... .make sure we clean up globals \n",
    "\n",
    "#Training iterations \n",
    "training_itr = 200\n",
    "batch_size = 50 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(training_itr):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        if i %100 ==0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_:batch[1] ,keep_prob:1.0}) #note we keep the whole network together to test \n",
    "            print('step %d training accuracy %g' % (i,train_accuracy))\n",
    "        #keep training \n",
    "        train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})#drop out half the network nodes \n",
    "    #Finally \n",
    "    print 'we are out'\n",
    "    # we run out of memory for the whole dataset? \n",
    "    my_batch = mnist.test.next_batch(10000)\n",
    "    \n",
    "    my_acc = accuracy.eval(feed_dict={x:my_batch[0],y_:my_batch[1],keep_prob:1.0})\n",
    "    \n",
    "    #my_acc = accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})\n",
    "    print 'past'\n",
    "    print my_acc\n",
    "    #print('total test accuracy %g' % accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
